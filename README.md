# ğŸ  AI Interior Redesign

Transform interior objects while preserving their geometry using SAM + ControlNet.

---

## ğŸ¯ What Does It Do?

Click on any furniture in a room photo, describe what material you want, and the AI will change it **without deforming the shape**.

**Problem I'm Solving:**  
Regular AI inpainting often changes object shapes. A pipeline combining **SAM** (segmentation) + **ControlNet** (guided generation) to preserve geometry while transforming materials.

---

## ğŸ¥ Demo

![Demo](assets/demo.gif)

*Click any object â†’ Describe the material â†’ Get photorealistic results*

---

## ğŸ¯ Key Features

- **ğŸ¯ Interactive Segmentation**: Click to select any furniture or decor item (powered by SAM)
- **ğŸ”’ Geometry Preservation**: ControlNet ensures shapes stay intact during material changes
- **ğŸ¨ Dual Control Modes**: 
  - **Depth-based**: Better for complex 3D shapes (sofas, chairs)
  - **Edge-based**: Better for sharp details (frames, tables)
- **âš¡ Real-time Preview**: Visual mask overlay shows selected areas
- **ğŸ’¾ Memory Efficient**: Optimized for 6GB VRAM through dynamic model loading
- **ğŸ­ Professional UI**: Clean Gradio interface with examples

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- CUDA GPU with 6GB+ VRAM (recommended)
- 10GB free disk space

### Installation

```bash
# Clone the repository
git clone https://github.com/ganart/interior-redesign.git
cd interior-redesign

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download SAM model (375MB)
wget -P models/ https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
```

### Run

```bash
python main.py
```

Opens at `http://localhost:7860`

---

## ğŸ’» Usage

### Basic Workflow

1. **Upload** a room photo
2. **Click** on the object you want to redesign
3. **Enter** a material description (e.g., "velvet purple fabric")
4. **Select** control mode (depth/canny)
5. **Generate**!

### Key Components

- **SAM** (Segment Anything): Click-based object segmentation
- **ControlNet**: Guides Stable Diffusion to preserve structure
- **Stable Diffusion 1.5**: Generates new textures
- **Gradio**: Simple web UI

---

## ğŸ“Š Examples

| Before | After                        | Prompt |
|--------|------------------------------|--------|
| ![](assets/example1.jpg) | ![](assets/example1_res.png) | "emerald green velvet ottoman" |
| ![](assets/example2.png) | ![](assets/example2_res.png) | "beige linen sofa" |

*Actual results depend on the prompt and object*

---

## ğŸ› ï¸ Technical Details

**Stack:**
- Python 3.10
- PyTorch 2.11
- Gradio 6.5.1
- 6GB VRAM (RTX 3060 Laptop)

**Memory Optimization:**
- Dynamic model loading/unloading
- Attention slicing
- Sequential processing

**Inference Time:**
- Segmentation: ~2-3 sec
- Generation: ~20-30 sec
- Total: ~25-35 sec

---

## ğŸ“ Project Structure

```
interior-redesign/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration & paths
â”‚   â”œâ”€â”€ utils.py           # Helper functions
â”‚   â”œâ”€â”€ segment.py         # SAM integration
â”‚   â”œâ”€â”€ inpainter.py       # SD + ControlNet pipeline
â”‚   â””â”€â”€ gradio_ui.py       # Web interface
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/             # User uploads
â”‚   â””â”€â”€ output/            # Generated results
â”œâ”€â”€ models/                # Model checkpoints
â”œâ”€â”€ assets/                # UI assets & examples
â”œâ”€â”€ main.py               # Entry point
â”œâ”€â”€ auth.py               # HuggingFace authentication
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ LICENSE              # MIT License
â””â”€â”€ README.md           # This file
```

---

## ğŸ“ What I Learned

- Integrating multiple large models (SAM, SD, ControlNet)
- Memory management for limited VRAM (6GB)
- Building interactive ML applications with Gradio
- Production code practices (logging, error handling, documentation)

---

## âš ï¸ Known Limitations

- Works best on well-lit, clear photos
- Small or partially hidden objects may not segment well
- Generation can take 30+ seconds
- Requires GPU for practical use

---

## ğŸ”§ Future Improvements

- [ ] Deploy to HuggingFace Spaces for live demo
- [ ] Add batch processing for multiple objects
- [ ] Improve generation speed and quality

---

## ğŸ“ Credits

Built with:
- [Segment Anything (SAM)](https://github.com/facebookresearch/segment-anything) by Meta AI
- [ControlNet](https://github.com/lllyasviel/ControlNet) by Lvmin Zhang
- [Stable Diffusion](https://github.com/huggingface/diffusers) via HuggingFace Diffusers

---

## ğŸ‘¤ Author

**Illia Hovorukha**

Applied Mathematics student at NTU "KhPI"  

- GitHub: [@ganart](https://github.com/ganart)
- Email: gr0nt0nbith@gmail.com

---

## ğŸ“„ License

MIT License - feel free to use for learning!

---

*This is a student project built as part of my portfolio. Feedback welcome!*